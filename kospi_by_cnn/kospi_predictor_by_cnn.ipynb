{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"saved_model_cnn1\"\n",
    "cols = [\"KOSPI\", \"KOSPI_START\", \"KOSPI_HIGH\", \"KOSPI_LOW\", \"NASDAQ\", \"DOW\", \"CR\", \"GOLD\"]\n",
    "len_x_ARMA = 60\n",
    "len_y_nextday = 1\n",
    "scale_method = \"minmax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>KOSPI</th>\n",
       "      <th>KOSPI_START</th>\n",
       "      <th>KOSPI_HIGH</th>\n",
       "      <th>KOSPI_LOW</th>\n",
       "      <th>NASDAQ</th>\n",
       "      <th>DOW</th>\n",
       "      <th>CR</th>\n",
       "      <th>GOLD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-03-25</td>\n",
       "      <td>2729.98</td>\n",
       "      <td>2740.83</td>\n",
       "      <td>2740.95</td>\n",
       "      <td>2722.36</td>\n",
       "      <td>14191.839</td>\n",
       "      <td>34707.94</td>\n",
       "      <td>1221.0</td>\n",
       "      <td>1956.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-03-24</td>\n",
       "      <td>2729.66</td>\n",
       "      <td>2716.25</td>\n",
       "      <td>2729.66</td>\n",
       "      <td>2705.14</td>\n",
       "      <td>13922.604</td>\n",
       "      <td>34358.50</td>\n",
       "      <td>1223.5</td>\n",
       "      <td>1941.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-03-23</td>\n",
       "      <td>2735.05</td>\n",
       "      <td>2727.12</td>\n",
       "      <td>2742.27</td>\n",
       "      <td>2720.83</td>\n",
       "      <td>14108.817</td>\n",
       "      <td>34807.46</td>\n",
       "      <td>1216.5</td>\n",
       "      <td>1926.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-03-22</td>\n",
       "      <td>2710.00</td>\n",
       "      <td>2686.68</td>\n",
       "      <td>2712.14</td>\n",
       "      <td>2686.58</td>\n",
       "      <td>13838.460</td>\n",
       "      <td>34552.99</td>\n",
       "      <td>1219.5</td>\n",
       "      <td>1926.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-03-21</td>\n",
       "      <td>2686.05</td>\n",
       "      <td>2717.81</td>\n",
       "      <td>2717.81</td>\n",
       "      <td>2683.69</td>\n",
       "      <td>13893.837</td>\n",
       "      <td>34754.93</td>\n",
       "      <td>1215.5</td>\n",
       "      <td>1925.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>2014-02-11</td>\n",
       "      <td>1932.06</td>\n",
       "      <td>1924.45</td>\n",
       "      <td>1935.29</td>\n",
       "      <td>1920.55</td>\n",
       "      <td>4148.174</td>\n",
       "      <td>15801.79</td>\n",
       "      <td>1067.0</td>\n",
       "      <td>1285.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>2014-02-10</td>\n",
       "      <td>1923.30</td>\n",
       "      <td>1928.13</td>\n",
       "      <td>1928.39</td>\n",
       "      <td>1916.40</td>\n",
       "      <td>4125.861</td>\n",
       "      <td>15794.08</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>1275.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>2014-02-07</td>\n",
       "      <td>1922.50</td>\n",
       "      <td>1922.45</td>\n",
       "      <td>1923.25</td>\n",
       "      <td>1910.90</td>\n",
       "      <td>4057.122</td>\n",
       "      <td>15628.53</td>\n",
       "      <td>1075.5</td>\n",
       "      <td>1261.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>2014-02-06</td>\n",
       "      <td>1907.89</td>\n",
       "      <td>1897.35</td>\n",
       "      <td>1910.91</td>\n",
       "      <td>1897.35</td>\n",
       "      <td>4011.552</td>\n",
       "      <td>15440.23</td>\n",
       "      <td>1079.0</td>\n",
       "      <td>1259.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>2014-02-05</td>\n",
       "      <td>1891.32</td>\n",
       "      <td>1897.95</td>\n",
       "      <td>1901.40</td>\n",
       "      <td>1888.47</td>\n",
       "      <td>4031.520</td>\n",
       "      <td>15445.24</td>\n",
       "      <td>1078.0</td>\n",
       "      <td>1257.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2001 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date    KOSPI  KOSPI_START  KOSPI_HIGH  KOSPI_LOW     NASDAQ  \\\n",
       "0     2022-03-25  2729.98      2740.83     2740.95    2722.36  14191.839   \n",
       "1     2022-03-24  2729.66      2716.25     2729.66    2705.14  13922.604   \n",
       "2     2022-03-23  2735.05      2727.12     2742.27    2720.83  14108.817   \n",
       "3     2022-03-22  2710.00      2686.68     2712.14    2686.58  13838.460   \n",
       "4     2022-03-21  2686.05      2717.81     2717.81    2683.69  13893.837   \n",
       "...          ...      ...          ...         ...        ...        ...   \n",
       "1996  2014-02-11  1932.06      1924.45     1935.29    1920.55   4148.174   \n",
       "1997  2014-02-10  1923.30      1928.13     1928.39    1916.40   4125.861   \n",
       "1998  2014-02-07  1922.50      1922.45     1923.25    1910.90   4057.122   \n",
       "1999  2014-02-06  1907.89      1897.35     1910.91    1897.35   4011.552   \n",
       "2000  2014-02-05  1891.32      1897.95     1901.40    1888.47   4031.520   \n",
       "\n",
       "           DOW      CR     GOLD  \n",
       "0     34707.94  1221.0  1956.45  \n",
       "1     34358.50  1223.5  1941.42  \n",
       "2     34807.46  1216.5  1926.22  \n",
       "3     34552.99  1219.5  1926.25  \n",
       "4     34754.93  1215.5  1925.90  \n",
       "...        ...     ...      ...  \n",
       "1996  15801.79  1067.0  1285.40  \n",
       "1997  15794.08  1072.0  1275.15  \n",
       "1998  15628.53  1075.5  1261.80  \n",
       "1999  15440.23  1079.0  1259.20  \n",
       "2000  15445.24  1078.0  1257.00  \n",
       "\n",
       "[2001 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kospi_predict import Crawler\n",
    "\n",
    "crawler = Crawler(crawl_page_max=30, perPage=100)\n",
    "# crawler.crawlData(cols, save=True)\n",
    "crawler.loadFromSavedFile(cols)\n",
    "df_crawled = crawler.removeNan()\n",
    "df_crawled = df_crawled.loc[:2000]\n",
    "df_crawled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "차분 생성중... (100.00%)\n",
      "\n",
      "자기상관 생성중... (100.00%)\n",
      "\n",
      "이동평균 생성중... (100.00%)\n",
      "\n",
      "라벨 생성중... (100.00%)\r"
     ]
    }
   ],
   "source": [
    "from kospi_predict import DataPreprocessor\n",
    "from sklearn.preprocessing import MinMaxScaler as MinMaxScaler\n",
    "\n",
    "dpp = DataPreprocessor(df_crawled, cols, scale_method, model_save_path)\n",
    "dpp.sortByDate()\n",
    "dpp.makeDiffByRange(1, len_x_ARMA)\n",
    "dpp.makeAR(0, len_x_ARMA)\n",
    "dpp.makeMA(2, len_x_ARMA)\n",
    "dpp.makeTargetYs(len_y_nextday)\n",
    "dpp.cutoffData(len_x_ARMA, len_y_nextday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>KOSPI</th>\n",
       "      <th>KOSPI_START</th>\n",
       "      <th>KOSPI_HIGH</th>\n",
       "      <th>KOSPI_LOW</th>\n",
       "      <th>NASDAQ</th>\n",
       "      <th>DOW</th>\n",
       "      <th>CR</th>\n",
       "      <th>GOLD</th>\n",
       "      <th>X_KOSPI_DIFF1</th>\n",
       "      <th>...</th>\n",
       "      <th>X_GOLD_MA53</th>\n",
       "      <th>X_GOLD_MA54</th>\n",
       "      <th>X_GOLD_MA55</th>\n",
       "      <th>X_GOLD_MA56</th>\n",
       "      <th>X_GOLD_MA57</th>\n",
       "      <th>X_GOLD_MA58</th>\n",
       "      <th>X_GOLD_MA59</th>\n",
       "      <th>X_GOLD_MA60</th>\n",
       "      <th>X_GOLD_MA61</th>\n",
       "      <th>Y_KOSPI_nextday_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-05-02</td>\n",
       "      <td>1959.44</td>\n",
       "      <td>1966.29</td>\n",
       "      <td>1968.50</td>\n",
       "      <td>1956.32</td>\n",
       "      <td>4127.451</td>\n",
       "      <td>16558.87</td>\n",
       "      <td>1030.3</td>\n",
       "      <td>1284.25</td>\n",
       "      <td>-2.35</td>\n",
       "      <td>...</td>\n",
       "      <td>1318.838679</td>\n",
       "      <td>1318.663889</td>\n",
       "      <td>1318.097273</td>\n",
       "      <td>1317.537500</td>\n",
       "      <td>1316.973684</td>\n",
       "      <td>1316.252586</td>\n",
       "      <td>1315.329661</td>\n",
       "      <td>1314.394167</td>\n",
       "      <td>1313.453279</td>\n",
       "      <td>1939.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-05-07</td>\n",
       "      <td>1939.88</td>\n",
       "      <td>1964.19</td>\n",
       "      <td>1964.19</td>\n",
       "      <td>1939.55</td>\n",
       "      <td>4080.759</td>\n",
       "      <td>16401.02</td>\n",
       "      <td>1023.9</td>\n",
       "      <td>1312.93</td>\n",
       "      <td>-19.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1318.064151</td>\n",
       "      <td>1318.198148</td>\n",
       "      <td>1318.038182</td>\n",
       "      <td>1317.492857</td>\n",
       "      <td>1316.953509</td>\n",
       "      <td>1316.409483</td>\n",
       "      <td>1315.710169</td>\n",
       "      <td>1314.811667</td>\n",
       "      <td>1313.900000</td>\n",
       "      <td>1950.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-05-08</td>\n",
       "      <td>1950.60</td>\n",
       "      <td>1945.94</td>\n",
       "      <td>1950.60</td>\n",
       "      <td>1934.72</td>\n",
       "      <td>4067.673</td>\n",
       "      <td>16518.54</td>\n",
       "      <td>1022.1</td>\n",
       "      <td>1292.00</td>\n",
       "      <td>10.72</td>\n",
       "      <td>...</td>\n",
       "      <td>1317.929245</td>\n",
       "      <td>1317.969074</td>\n",
       "      <td>1318.102364</td>\n",
       "      <td>1317.946964</td>\n",
       "      <td>1317.412807</td>\n",
       "      <td>1316.884138</td>\n",
       "      <td>1316.350508</td>\n",
       "      <td>1315.663833</td>\n",
       "      <td>1314.780820</td>\n",
       "      <td>1956.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-05-09</td>\n",
       "      <td>1956.55</td>\n",
       "      <td>1953.48</td>\n",
       "      <td>1958.34</td>\n",
       "      <td>1949.64</td>\n",
       "      <td>4051.496</td>\n",
       "      <td>16550.97</td>\n",
       "      <td>1025.5</td>\n",
       "      <td>1289.30</td>\n",
       "      <td>5.95</td>\n",
       "      <td>...</td>\n",
       "      <td>1317.380189</td>\n",
       "      <td>1317.449074</td>\n",
       "      <td>1317.496909</td>\n",
       "      <td>1317.636250</td>\n",
       "      <td>1317.491754</td>\n",
       "      <td>1316.974655</td>\n",
       "      <td>1316.462373</td>\n",
       "      <td>1315.944667</td>\n",
       "      <td>1315.275902</td>\n",
       "      <td>1964.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-05-12</td>\n",
       "      <td>1964.94</td>\n",
       "      <td>1956.26</td>\n",
       "      <td>1966.64</td>\n",
       "      <td>1950.91</td>\n",
       "      <td>4071.870</td>\n",
       "      <td>16583.34</td>\n",
       "      <td>1024.7</td>\n",
       "      <td>1291.75</td>\n",
       "      <td>8.39</td>\n",
       "      <td>...</td>\n",
       "      <td>1316.899057</td>\n",
       "      <td>1316.860185</td>\n",
       "      <td>1316.937273</td>\n",
       "      <td>1316.993393</td>\n",
       "      <td>1317.139123</td>\n",
       "      <td>1317.005690</td>\n",
       "      <td>1316.505593</td>\n",
       "      <td>1316.009667</td>\n",
       "      <td>1315.507869</td>\n",
       "      <td>1982.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934</th>\n",
       "      <td>2022-03-18</td>\n",
       "      <td>2707.02</td>\n",
       "      <td>2695.64</td>\n",
       "      <td>2708.16</td>\n",
       "      <td>2688.71</td>\n",
       "      <td>13614.781</td>\n",
       "      <td>34480.76</td>\n",
       "      <td>1215.5</td>\n",
       "      <td>1932.95</td>\n",
       "      <td>12.51</td>\n",
       "      <td>...</td>\n",
       "      <td>1861.692264</td>\n",
       "      <td>1860.687222</td>\n",
       "      <td>1859.712000</td>\n",
       "      <td>1858.396250</td>\n",
       "      <td>1857.319474</td>\n",
       "      <td>1856.344138</td>\n",
       "      <td>1855.520508</td>\n",
       "      <td>1854.369333</td>\n",
       "      <td>1852.957213</td>\n",
       "      <td>2686.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935</th>\n",
       "      <td>2022-03-21</td>\n",
       "      <td>2686.05</td>\n",
       "      <td>2717.81</td>\n",
       "      <td>2717.81</td>\n",
       "      <td>2683.69</td>\n",
       "      <td>13893.837</td>\n",
       "      <td>34754.93</td>\n",
       "      <td>1215.5</td>\n",
       "      <td>1925.90</td>\n",
       "      <td>-20.97</td>\n",
       "      <td>...</td>\n",
       "      <td>1864.082264</td>\n",
       "      <td>1863.011852</td>\n",
       "      <td>1862.001091</td>\n",
       "      <td>1861.019821</td>\n",
       "      <td>1859.704211</td>\n",
       "      <td>1858.623448</td>\n",
       "      <td>1857.642542</td>\n",
       "      <td>1856.811000</td>\n",
       "      <td>1855.657541</td>\n",
       "      <td>2710.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1936</th>\n",
       "      <td>2022-03-22</td>\n",
       "      <td>2710.00</td>\n",
       "      <td>2686.68</td>\n",
       "      <td>2712.14</td>\n",
       "      <td>2686.58</td>\n",
       "      <td>13838.460</td>\n",
       "      <td>34552.99</td>\n",
       "      <td>1219.5</td>\n",
       "      <td>1926.25</td>\n",
       "      <td>23.95</td>\n",
       "      <td>...</td>\n",
       "      <td>1866.167170</td>\n",
       "      <td>1865.227037</td>\n",
       "      <td>1864.155273</td>\n",
       "      <td>1863.142143</td>\n",
       "      <td>1862.158070</td>\n",
       "      <td>1860.845517</td>\n",
       "      <td>1859.763729</td>\n",
       "      <td>1858.780167</td>\n",
       "      <td>1857.943607</td>\n",
       "      <td>2735.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>2022-03-23</td>\n",
       "      <td>2735.05</td>\n",
       "      <td>2727.12</td>\n",
       "      <td>2742.27</td>\n",
       "      <td>2720.83</td>\n",
       "      <td>14108.817</td>\n",
       "      <td>34807.46</td>\n",
       "      <td>1216.5</td>\n",
       "      <td>1926.22</td>\n",
       "      <td>25.05</td>\n",
       "      <td>...</td>\n",
       "      <td>1868.486038</td>\n",
       "      <td>1867.279815</td>\n",
       "      <td>1866.336545</td>\n",
       "      <td>1865.264107</td>\n",
       "      <td>1864.249298</td>\n",
       "      <td>1863.263103</td>\n",
       "      <td>1861.954068</td>\n",
       "      <td>1860.871833</td>\n",
       "      <td>1859.886230</td>\n",
       "      <td>2729.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1938</th>\n",
       "      <td>2022-03-24</td>\n",
       "      <td>2729.66</td>\n",
       "      <td>2716.25</td>\n",
       "      <td>2729.66</td>\n",
       "      <td>2705.14</td>\n",
       "      <td>13922.604</td>\n",
       "      <td>34358.50</td>\n",
       "      <td>1223.5</td>\n",
       "      <td>1941.42</td>\n",
       "      <td>-5.39</td>\n",
       "      <td>...</td>\n",
       "      <td>1870.834717</td>\n",
       "      <td>1869.555185</td>\n",
       "      <td>1868.351455</td>\n",
       "      <td>1867.405893</td>\n",
       "      <td>1866.333509</td>\n",
       "      <td>1865.317759</td>\n",
       "      <td>1864.330169</td>\n",
       "      <td>1863.025167</td>\n",
       "      <td>1861.943115</td>\n",
       "      <td>2729.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1939 rows × 1450 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date    KOSPI  KOSPI_START  KOSPI_HIGH  KOSPI_LOW     NASDAQ  \\\n",
       "0     2014-05-02  1959.44      1966.29     1968.50    1956.32   4127.451   \n",
       "1     2014-05-07  1939.88      1964.19     1964.19    1939.55   4080.759   \n",
       "2     2014-05-08  1950.60      1945.94     1950.60    1934.72   4067.673   \n",
       "3     2014-05-09  1956.55      1953.48     1958.34    1949.64   4051.496   \n",
       "4     2014-05-12  1964.94      1956.26     1966.64    1950.91   4071.870   \n",
       "...          ...      ...          ...         ...        ...        ...   \n",
       "1934  2022-03-18  2707.02      2695.64     2708.16    2688.71  13614.781   \n",
       "1935  2022-03-21  2686.05      2717.81     2717.81    2683.69  13893.837   \n",
       "1936  2022-03-22  2710.00      2686.68     2712.14    2686.58  13838.460   \n",
       "1937  2022-03-23  2735.05      2727.12     2742.27    2720.83  14108.817   \n",
       "1938  2022-03-24  2729.66      2716.25     2729.66    2705.14  13922.604   \n",
       "\n",
       "           DOW      CR     GOLD  X_KOSPI_DIFF1  ...  X_GOLD_MA53  X_GOLD_MA54  \\\n",
       "0     16558.87  1030.3  1284.25          -2.35  ...  1318.838679  1318.663889   \n",
       "1     16401.02  1023.9  1312.93         -19.56  ...  1318.064151  1318.198148   \n",
       "2     16518.54  1022.1  1292.00          10.72  ...  1317.929245  1317.969074   \n",
       "3     16550.97  1025.5  1289.30           5.95  ...  1317.380189  1317.449074   \n",
       "4     16583.34  1024.7  1291.75           8.39  ...  1316.899057  1316.860185   \n",
       "...        ...     ...      ...            ...  ...          ...          ...   \n",
       "1934  34480.76  1215.5  1932.95          12.51  ...  1861.692264  1860.687222   \n",
       "1935  34754.93  1215.5  1925.90         -20.97  ...  1864.082264  1863.011852   \n",
       "1936  34552.99  1219.5  1926.25          23.95  ...  1866.167170  1865.227037   \n",
       "1937  34807.46  1216.5  1926.22          25.05  ...  1868.486038  1867.279815   \n",
       "1938  34358.50  1223.5  1941.42          -5.39  ...  1870.834717  1869.555185   \n",
       "\n",
       "      X_GOLD_MA55  X_GOLD_MA56  X_GOLD_MA57  X_GOLD_MA58  X_GOLD_MA59  \\\n",
       "0     1318.097273  1317.537500  1316.973684  1316.252586  1315.329661   \n",
       "1     1318.038182  1317.492857  1316.953509  1316.409483  1315.710169   \n",
       "2     1318.102364  1317.946964  1317.412807  1316.884138  1316.350508   \n",
       "3     1317.496909  1317.636250  1317.491754  1316.974655  1316.462373   \n",
       "4     1316.937273  1316.993393  1317.139123  1317.005690  1316.505593   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "1934  1859.712000  1858.396250  1857.319474  1856.344138  1855.520508   \n",
       "1935  1862.001091  1861.019821  1859.704211  1858.623448  1857.642542   \n",
       "1936  1864.155273  1863.142143  1862.158070  1860.845517  1859.763729   \n",
       "1937  1866.336545  1865.264107  1864.249298  1863.263103  1861.954068   \n",
       "1938  1868.351455  1867.405893  1866.333509  1865.317759  1864.330169   \n",
       "\n",
       "      X_GOLD_MA60  X_GOLD_MA61  Y_KOSPI_nextday_1  \n",
       "0     1314.394167  1313.453279            1939.88  \n",
       "1     1314.811667  1313.900000            1950.60  \n",
       "2     1315.663833  1314.780820            1956.55  \n",
       "3     1315.944667  1315.275902            1964.94  \n",
       "4     1316.009667  1315.507869            1982.93  \n",
       "...           ...          ...                ...  \n",
       "1934  1854.369333  1852.957213            2686.05  \n",
       "1935  1856.811000  1855.657541            2710.00  \n",
       "1936  1858.780167  1857.943607            2735.05  \n",
       "1937  1860.871833  1859.886230            2729.66  \n",
       "1938  1863.025167  1861.943115            2729.98  \n",
       "\n",
       "[1939 rows x 1450 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = dpp.df[500:1500]\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "dpp.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1938, 8, 60, 3), (1938,), (1938,), (1938, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def rescalingByMinMax(arr):\n",
    "    result = []\n",
    "    max_val = max(arr)\n",
    "    min_val = min(arr)\n",
    "    for val in arr:\n",
    "        new_val = (val - min_val) / (max_val - min_val)\n",
    "        result.append(new_val)\n",
    "    return result\n",
    "\n",
    "def makeImage3D(df):\n",
    "    img_list = []\n",
    "    label_list = []\n",
    "    date_list = []\n",
    "    y_list = []\n",
    "    for item in df.iterrows():\n",
    "        this_mat = []\n",
    "        for col in cols:\n",
    "            this_col_ar_mat = rescalingByMinMax(df.loc[item[0], [x for x in df.columns.to_list() if \"{}_AR\".format(col) in x]].to_list()) #AR\n",
    "            this_col_ma_mat = rescalingByMinMax(df.loc[item[0], [x for x in df.columns.to_list() if \"{}_MA\".format(col) in x]].to_list()) #MA\n",
    "            this_col_diff_mat = rescalingByMinMax(df.loc[item[0], [x for x in df.columns.to_list() if \"{}_DIFF\".format(col) in x]].to_list()) #D\n",
    "            this_row = []\n",
    "            for idx, _ in enumerate(this_col_ar_mat):\n",
    "                this_row.append([this_col_ar_mat[idx], this_col_ma_mat[idx], this_col_diff_mat[idx]])\n",
    "            this_mat.append(this_row)\n",
    "        this_mat = np.array(this_mat)\n",
    "        img_list.append(this_mat)\n",
    "\n",
    "        #라벨 체크\n",
    "        this_label = np.nan\n",
    "        if item[0]+1 < len(df): #0 1 2 ... 29 / 30\n",
    "            this_label = 0\n",
    "            if df.loc[item[0]+1, \"KOSPI\"] > df.loc[item[0], \"KOSPI\"]: #item[0]+1 : 다음날\n",
    "                this_label = 1\n",
    "        label_list.append(this_label)\n",
    "\n",
    "        #라벨값과 함께 이미지로 저장\n",
    "        cv2.imwrite(\"data_cnn/{}_{}.png\".format(str(this_label), df.loc[item[0], \"date\"]), this_mat * 255)\n",
    "\n",
    "        #날짜 리스트 입력\n",
    "        date_list.append(df.loc[item[0], \"date\"])\n",
    "\n",
    "        #y_list 입력\n",
    "        this_y_list = []\n",
    "        for y in [x for x in df.columns.to_list() if \"Y_\" in x]:\n",
    "            this_y_list.append(df.loc[item[0], y])\n",
    "        y_list.append(this_y_list)\n",
    "\n",
    "    img_list = np.array(img_list[:-1])\n",
    "    label_list = np.array(label_list[:-1], dtype=np.uint8)\n",
    "    date_list = np.array(date_list[:-1])\n",
    "    y_list = np.array(y_list[:-1])\n",
    "    df_result = pd.DataFrame({\"date\" : date_list, \"label\" : label_list})\n",
    "\n",
    "    return img_list, label_list, date_list, y_list, df_result\n",
    "\n",
    "img_list, label_list, date_list, y_list, df_ref = makeImage3D(dpp.df)\n",
    "img_list.shape, label_list.shape, date_list.shape, y_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-05-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-05-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-05-08</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-05-09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-05-12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933</th>\n",
       "      <td>2022-03-17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934</th>\n",
       "      <td>2022-03-18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935</th>\n",
       "      <td>2022-03-21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1936</th>\n",
       "      <td>2022-03-22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>2022-03-23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1938 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  label\n",
       "0     2014-05-02      0\n",
       "1     2014-05-07      1\n",
       "2     2014-05-08      1\n",
       "3     2014-05-09      1\n",
       "4     2014-05-12      1\n",
       "...          ...    ...\n",
       "1933  2022-03-17      1\n",
       "1934  2022-03-18      0\n",
       "1935  2022-03-21      1\n",
       "1936  2022-03-22      1\n",
       "1937  2022-03-23      0\n",
       "\n",
       "[1938 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeImage2D(df):\n",
    "    #행 : X_*_DIFF_AR, X_*_DIFF_MA  *  원천 X 갯수(8개라면 8*2 = 16행)\n",
    "    #열 : ARMA 갯수. 20이라면 20 = 20열\n",
    "    col_made_list = [\n",
    "        \"X_*_AR\", #8행\n",
    "        \"X_*_DIFF_AR\", #8행\n",
    "        \"X_*_MA\", #8행\n",
    "        \"X_*_DIFF_MA\" #8행\n",
    "    ]\n",
    "    img_list = []\n",
    "    label_list = []\n",
    "    date_list = []\n",
    "    y_list = []\n",
    "    for item in df.iterrows():\n",
    "        this_mat_list = []\n",
    "        for col in cols: #8개 [\"KOSPI\", \"KOSPI_START\", \"KOSPI_HIGH\", \"KOSPI_LOW\", \"NASDAQ\", \"DOW\", \"CR\", \"GOLD\"]\n",
    "            for col_made in col_made_list: #4개 [\"X_*_AR\", \"X_*_DIFF_AR\", \"X_*_MA\", \"X_*_DIFF_MA\"]\n",
    "                this_col = col_made.replace(\"*\", col) #X_KOSPI_AR\n",
    "                this_row_list = []\n",
    "                for i in range(0, len_x_ARMA): #20개. 0~20\n",
    "                    if \"MA\" in this_col:\n",
    "                        i += 2\n",
    "                    this_col_made = \"{}{}\".format(this_col, i) #X_KOSPI_AR0\n",
    "                    this_row_list.append([df.loc[item[0], this_col_made]])\n",
    "                this_mat_list.append(this_row_list)\n",
    "        this_mat = np.array(this_mat_list)\n",
    "        this_mat = this_mat\n",
    "        img_list.append(this_mat)\n",
    "\n",
    "        #라벨 체크\n",
    "        this_label = np.nan\n",
    "        if item[0]+1 < len(df): #0 1 2 ... 29 / 30\n",
    "            this_label = 0\n",
    "            if df.loc[item[0]+1, \"KOSPI\"] > df.loc[item[0], \"KOSPI\"]: #item[0]+1 : 다음날\n",
    "                this_label = 1\n",
    "        label_list.append(this_label)\n",
    "\n",
    "        #라벨값과 함께 이미지로 저장\n",
    "        cv2.imwrite(\"data_cnn/{}_{}.png\".format(str(this_label), df.loc[item[0], \"date\"]), this_mat * 255)\n",
    "\n",
    "        #날짜 리스트 입력\n",
    "        date_list.append(df.loc[item[0], \"date\"])\n",
    "\n",
    "        #y_list 입력\n",
    "        this_y_list = []\n",
    "        for y in [x for x in df.columns.to_list() if \"Y_\" in x]:\n",
    "            this_y_list.append(df.loc[item[0], y])\n",
    "        y_list.append(this_y_list)\n",
    "\n",
    "    img_list = np.array(img_list)\n",
    "    label_list = np.array(label_list)\n",
    "    date_list = np.array(date_list)\n",
    "    y_list = np.array(y_list)\n",
    "    df_result = pd.DataFrame({\"date\" : date_list, \"label\" : label_list})\n",
    "\n",
    "    return img_list, label_list, date_list, y_list, df_result\n",
    "\n",
    "# img_list, label_list, date_list, y_list, _ = makeImage2D(dpp.df)\n",
    "# img_list.shape, label_list.shape, date_list.shape, y_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1549, 8, 60, 3), (388, 8, 60, 3), (1549,), (388,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(img_list[:-1], label_list[:-1], test_size=0.2, shuffle=False, random_state=8699)\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 8, 60, 16)         448       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 8, 60, 32)         4640      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 4, 30, 32)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 4, 30, 64)         18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 2, 15, 64)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 2, 15, 128)        73856     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3840)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               491648    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 589,346\n",
      "Trainable params: 589,346\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "39/39 [==============================] - 4s 26ms/step - loss: 0.7029 - accuracy: 0.5117 - val_loss: 0.6936 - val_accuracy: 0.4258\n",
      "Epoch 2/200\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 0.6929 - accuracy: 0.5085 - val_loss: 0.6890 - val_accuracy: 0.5742\n",
      "Epoch 3/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.6914 - accuracy: 0.5238 - val_loss: 0.6863 - val_accuracy: 0.5774\n",
      "Epoch 4/200\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.6937 - accuracy: 0.5262 - val_loss: 0.6886 - val_accuracy: 0.5613\n",
      "Epoch 5/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.6909 - accuracy: 0.5327 - val_loss: 0.6888 - val_accuracy: 0.5548\n",
      "Epoch 6/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.6875 - accuracy: 0.5480 - val_loss: 0.6923 - val_accuracy: 0.5032\n",
      "Epoch 7/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.6840 - accuracy: 0.5601 - val_loss: 0.7018 - val_accuracy: 0.5613\n",
      "Epoch 8/200\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 0.6842 - accuracy: 0.5569 - val_loss: 0.6924 - val_accuracy: 0.5161\n",
      "Epoch 9/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.6820 - accuracy: 0.5658 - val_loss: 0.6991 - val_accuracy: 0.5226\n",
      "Epoch 10/200\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.6731 - accuracy: 0.5779 - val_loss: 0.6946 - val_accuracy: 0.5484\n",
      "Epoch 11/200\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 0.6799 - accuracy: 0.5738 - val_loss: 0.7009 - val_accuracy: 0.5710\n",
      "Epoch 12/200\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 0.6664 - accuracy: 0.5924 - val_loss: 0.7223 - val_accuracy: 0.4645\n",
      "Epoch 13/200\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.6596 - accuracy: 0.5989 - val_loss: 0.7160 - val_accuracy: 0.5129\n",
      "Epoch 14/200\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.6545 - accuracy: 0.6118 - val_loss: 0.7129 - val_accuracy: 0.5161\n",
      "Epoch 15/200\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 0.6477 - accuracy: 0.6271 - val_loss: 0.7419 - val_accuracy: 0.4774\n",
      "Epoch 16/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.6359 - accuracy: 0.6247 - val_loss: 0.7650 - val_accuracy: 0.4742\n",
      "Epoch 17/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.6318 - accuracy: 0.6287 - val_loss: 0.7343 - val_accuracy: 0.4839\n",
      "Epoch 18/200\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.6196 - accuracy: 0.6562 - val_loss: 0.7695 - val_accuracy: 0.4742\n",
      "Epoch 19/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.6093 - accuracy: 0.6521 - val_loss: 0.7309 - val_accuracy: 0.5258\n",
      "Epoch 20/200\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 0.5991 - accuracy: 0.6602 - val_loss: 0.8879 - val_accuracy: 0.5194\n",
      "Epoch 21/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.5910 - accuracy: 0.6796 - val_loss: 0.8477 - val_accuracy: 0.4710\n",
      "Epoch 22/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.5586 - accuracy: 0.6981 - val_loss: 0.8271 - val_accuracy: 0.4419\n",
      "Epoch 23/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.5614 - accuracy: 0.6973 - val_loss: 0.8316 - val_accuracy: 0.5129\n",
      "Epoch 24/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.5386 - accuracy: 0.7199 - val_loss: 0.8146 - val_accuracy: 0.4968\n",
      "Epoch 25/200\n",
      "39/39 [==============================] - 1s 17ms/step - loss: 0.5084 - accuracy: 0.7425 - val_loss: 0.9324 - val_accuracy: 0.4774\n",
      "Epoch 26/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.4971 - accuracy: 0.7353 - val_loss: 0.8633 - val_accuracy: 0.4968\n",
      "Epoch 27/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.4456 - accuracy: 0.7861 - val_loss: 0.9911 - val_accuracy: 0.5032\n",
      "Epoch 28/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.4165 - accuracy: 0.8063 - val_loss: 1.0023 - val_accuracy: 0.4516\n",
      "Epoch 29/200\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.3766 - accuracy: 0.8232 - val_loss: 1.2031 - val_accuracy: 0.4903\n",
      "Epoch 30/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.3524 - accuracy: 0.8394 - val_loss: 1.1945 - val_accuracy: 0.4806\n",
      "Epoch 31/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.2970 - accuracy: 0.8717 - val_loss: 1.3544 - val_accuracy: 0.4839\n",
      "Epoch 32/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.2817 - accuracy: 0.8830 - val_loss: 1.4529 - val_accuracy: 0.4806\n",
      "Epoch 33/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.2294 - accuracy: 0.9104 - val_loss: 1.6866 - val_accuracy: 0.5129\n",
      "Epoch 34/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.1896 - accuracy: 0.9266 - val_loss: 1.8737 - val_accuracy: 0.5194\n",
      "Epoch 35/200\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 0.2099 - accuracy: 0.9185 - val_loss: 1.6483 - val_accuracy: 0.4935\n",
      "Epoch 36/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.1458 - accuracy: 0.9492 - val_loss: 1.9495 - val_accuracy: 0.5387\n",
      "Epoch 37/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.1189 - accuracy: 0.9556 - val_loss: 2.1419 - val_accuracy: 0.4968\n",
      "Epoch 38/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.0919 - accuracy: 0.9645 - val_loss: 2.2944 - val_accuracy: 0.5000\n",
      "Epoch 39/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.0784 - accuracy: 0.9693 - val_loss: 2.2474 - val_accuracy: 0.4806\n",
      "Epoch 40/200\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.0756 - accuracy: 0.9774 - val_loss: 2.5080 - val_accuracy: 0.4774\n",
      "Epoch 41/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.0607 - accuracy: 0.9814 - val_loss: 2.5887 - val_accuracy: 0.4677\n",
      "Epoch 42/200\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 0.0370 - accuracy: 0.9895 - val_loss: 2.9765 - val_accuracy: 0.4548\n",
      "Epoch 43/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.0279 - accuracy: 0.9919 - val_loss: 2.8351 - val_accuracy: 0.5000\n",
      "Epoch 44/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.0215 - accuracy: 0.9911 - val_loss: 2.9182 - val_accuracy: 0.4903\n",
      "Epoch 45/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.0431 - accuracy: 0.9871 - val_loss: 2.7981 - val_accuracy: 0.4742\n",
      "Epoch 46/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.0410 - accuracy: 0.9855 - val_loss: 2.8566 - val_accuracy: 0.5226\n",
      "Epoch 47/200\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.0349 - accuracy: 0.9903 - val_loss: 3.0182 - val_accuracy: 0.5065\n",
      "Epoch 48/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.0133 - accuracy: 0.9968 - val_loss: 3.2242 - val_accuracy: 0.5065\n",
      "Epoch 49/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.0067 - accuracy: 0.9992 - val_loss: 3.7598 - val_accuracy: 0.4871\n",
      "Epoch 50/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 3.8303 - val_accuracy: 0.5065\n",
      "Epoch 51/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 3.9088 - val_accuracy: 0.4968\n",
      "Epoch 52/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 6.0061e-04 - accuracy: 1.0000 - val_loss: 3.9656 - val_accuracy: 0.5000\n",
      "Epoch 53/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 4.8374e-04 - accuracy: 1.0000 - val_loss: 4.0075 - val_accuracy: 0.5032\n",
      "Epoch 54/200\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 4.1942e-04 - accuracy: 1.0000 - val_loss: 4.0561 - val_accuracy: 0.5032\n",
      "Epoch 55/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 3.7543e-04 - accuracy: 1.0000 - val_loss: 4.0879 - val_accuracy: 0.5065\n",
      "Epoch 56/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 3.4117e-04 - accuracy: 1.0000 - val_loss: 4.1286 - val_accuracy: 0.5032\n",
      "Epoch 57/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 3.1165e-04 - accuracy: 1.0000 - val_loss: 4.1606 - val_accuracy: 0.5097\n",
      "Epoch 58/200\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 2.8755e-04 - accuracy: 1.0000 - val_loss: 4.1887 - val_accuracy: 0.5097\n",
      "Epoch 59/200\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 2.6605e-04 - accuracy: 1.0000 - val_loss: 4.2169 - val_accuracy: 0.5097\n",
      "Epoch 60/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 2.4886e-04 - accuracy: 1.0000 - val_loss: 4.2420 - val_accuracy: 0.5065\n",
      "Epoch 61/200\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 2.3328e-04 - accuracy: 1.0000 - val_loss: 4.2689 - val_accuracy: 0.5065\n",
      "Epoch 62/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 2.1909e-04 - accuracy: 1.0000 - val_loss: 4.2924 - val_accuracy: 0.5065\n",
      "Epoch 63/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 2.0770e-04 - accuracy: 1.0000 - val_loss: 4.3177 - val_accuracy: 0.5065\n",
      "Epoch 64/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 1.9535e-04 - accuracy: 1.0000 - val_loss: 4.3415 - val_accuracy: 0.5065\n",
      "Epoch 65/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 1.8413e-04 - accuracy: 1.0000 - val_loss: 4.3635 - val_accuracy: 0.5065\n",
      "Epoch 66/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 1.7517e-04 - accuracy: 1.0000 - val_loss: 4.3857 - val_accuracy: 0.5032\n",
      "Epoch 67/200\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 1.6608e-04 - accuracy: 1.0000 - val_loss: 4.4060 - val_accuracy: 0.5032\n",
      "Epoch 68/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 1.5855e-04 - accuracy: 1.0000 - val_loss: 4.4302 - val_accuracy: 0.5032\n",
      "Epoch 69/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 1.5043e-04 - accuracy: 1.0000 - val_loss: 4.4508 - val_accuracy: 0.5032\n",
      "Epoch 70/200\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 1.4310e-04 - accuracy: 1.0000 - val_loss: 4.4690 - val_accuracy: 0.5032\n",
      "Epoch 71/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 1.3678e-04 - accuracy: 1.0000 - val_loss: 4.4911 - val_accuracy: 0.5065\n",
      "Epoch 72/200\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 1.3051e-04 - accuracy: 1.0000 - val_loss: 4.5150 - val_accuracy: 0.5065\n",
      "Epoch 73/200\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 1.2501e-04 - accuracy: 1.0000 - val_loss: 4.5343 - val_accuracy: 0.5065\n",
      "Epoch 74/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 1.1878e-04 - accuracy: 1.0000 - val_loss: 4.5535 - val_accuracy: 0.5097\n",
      "Epoch 75/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 1.1330e-04 - accuracy: 1.0000 - val_loss: 4.5721 - val_accuracy: 0.5097\n",
      "Epoch 76/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 1.0922e-04 - accuracy: 1.0000 - val_loss: 4.5919 - val_accuracy: 0.5097\n",
      "Epoch 77/200\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 1.0376e-04 - accuracy: 1.0000 - val_loss: 4.6085 - val_accuracy: 0.5065\n",
      "Epoch 78/200\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 9.8234e-05 - accuracy: 1.0000 - val_loss: 4.6286 - val_accuracy: 0.5000\n",
      "Epoch 79/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 9.4487e-05 - accuracy: 1.0000 - val_loss: 4.6454 - val_accuracy: 0.5000\n",
      "Epoch 80/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 8.9861e-05 - accuracy: 1.0000 - val_loss: 4.6657 - val_accuracy: 0.5000\n",
      "Epoch 81/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 8.5772e-05 - accuracy: 1.0000 - val_loss: 4.6838 - val_accuracy: 0.5032\n",
      "Epoch 82/200\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 8.2118e-05 - accuracy: 1.0000 - val_loss: 4.7003 - val_accuracy: 0.4968\n",
      "Epoch 83/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 7.8602e-05 - accuracy: 1.0000 - val_loss: 4.7174 - val_accuracy: 0.5000\n",
      "Epoch 84/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 7.5717e-05 - accuracy: 1.0000 - val_loss: 4.7364 - val_accuracy: 0.5097\n",
      "Epoch 85/200\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 7.2381e-05 - accuracy: 1.0000 - val_loss: 4.7535 - val_accuracy: 0.5032\n",
      "Epoch 86/200\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 6.9316e-05 - accuracy: 1.0000 - val_loss: 4.7678 - val_accuracy: 0.5032\n",
      "Epoch 87/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 6.6118e-05 - accuracy: 1.0000 - val_loss: 4.7871 - val_accuracy: 0.4968\n",
      "Epoch 88/200\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 6.3160e-05 - accuracy: 1.0000 - val_loss: 4.8015 - val_accuracy: 0.4968\n",
      "Epoch 89/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 6.0708e-05 - accuracy: 1.0000 - val_loss: 4.8189 - val_accuracy: 0.5065\n",
      "Epoch 90/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 5.8078e-05 - accuracy: 1.0000 - val_loss: 4.8341 - val_accuracy: 0.4968\n",
      "Epoch 91/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 5.5872e-05 - accuracy: 1.0000 - val_loss: 4.8460 - val_accuracy: 0.4935\n",
      "Epoch 92/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 5.3362e-05 - accuracy: 1.0000 - val_loss: 4.8618 - val_accuracy: 0.5000\n",
      "Epoch 93/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 5.1503e-05 - accuracy: 1.0000 - val_loss: 4.8798 - val_accuracy: 0.5000\n",
      "Epoch 94/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 4.9866e-05 - accuracy: 1.0000 - val_loss: 4.8929 - val_accuracy: 0.5000\n",
      "Epoch 95/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 4.7570e-05 - accuracy: 1.0000 - val_loss: 4.9094 - val_accuracy: 0.5032\n",
      "Epoch 96/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 4.6166e-05 - accuracy: 1.0000 - val_loss: 4.9271 - val_accuracy: 0.5000\n",
      "Epoch 97/200\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 4.4063e-05 - accuracy: 1.0000 - val_loss: 4.9368 - val_accuracy: 0.5065\n",
      "Epoch 98/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 4.2673e-05 - accuracy: 1.0000 - val_loss: 4.9499 - val_accuracy: 0.5032\n",
      "Epoch 99/200\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 4.0998e-05 - accuracy: 1.0000 - val_loss: 4.9685 - val_accuracy: 0.5032\n",
      "Epoch 100/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 3.9475e-05 - accuracy: 1.0000 - val_loss: 4.9835 - val_accuracy: 0.5032\n",
      "Epoch 101/200\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 3.7895e-05 - accuracy: 1.0000 - val_loss: 4.9999 - val_accuracy: 0.5065\n",
      "Epoch 102/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 3.6704e-05 - accuracy: 1.0000 - val_loss: 5.0139 - val_accuracy: 0.5032\n",
      "Epoch 103/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 3.5111e-05 - accuracy: 1.0000 - val_loss: 5.0195 - val_accuracy: 0.5032\n",
      "Epoch 104/200\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 3.3932e-05 - accuracy: 1.0000 - val_loss: 5.0446 - val_accuracy: 0.5000\n",
      "Epoch 105/200\n",
      "39/39 [==============================] - 1s 13ms/step - loss: 3.2878e-05 - accuracy: 1.0000 - val_loss: 5.0518 - val_accuracy: 0.4968\n",
      "Epoch 106/200\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 3.1388e-05 - accuracy: 1.0000 - val_loss: 5.0669 - val_accuracy: 0.5065\n",
      "Epoch 107/200\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 3.0460e-05 - accuracy: 1.0000 - val_loss: 5.0793 - val_accuracy: 0.5065\n",
      "Epoch 108/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 2.9319e-05 - accuracy: 1.0000 - val_loss: 5.0994 - val_accuracy: 0.4968\n",
      "Epoch 109/200\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 2.8399e-05 - accuracy: 1.0000 - val_loss: 5.1035 - val_accuracy: 0.5000\n",
      "Epoch 110/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 2.7336e-05 - accuracy: 1.0000 - val_loss: 5.1197 - val_accuracy: 0.5065\n",
      "Epoch 111/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 2.6316e-05 - accuracy: 1.0000 - val_loss: 5.1282 - val_accuracy: 0.5032\n",
      "Epoch 112/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 2.5357e-05 - accuracy: 1.0000 - val_loss: 5.1493 - val_accuracy: 0.4968\n",
      "Epoch 113/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 2.4494e-05 - accuracy: 1.0000 - val_loss: 5.1573 - val_accuracy: 0.4968\n",
      "Epoch 114/200\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 2.3569e-05 - accuracy: 1.0000 - val_loss: 5.1679 - val_accuracy: 0.4968\n",
      "Epoch 115/200\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 2.2760e-05 - accuracy: 1.0000 - val_loss: 5.1811 - val_accuracy: 0.4968\n",
      "Epoch 116/200\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 2.2144e-05 - accuracy: 1.0000 - val_loss: 5.1896 - val_accuracy: 0.4968\n",
      "Epoch 117/200\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 2.1209e-05 - accuracy: 1.0000 - val_loss: 5.2073 - val_accuracy: 0.4935\n",
      "Epoch 118/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 2.0844e-05 - accuracy: 1.0000 - val_loss: 5.2098 - val_accuracy: 0.5032\n",
      "Epoch 119/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 1.9993e-05 - accuracy: 1.0000 - val_loss: 5.2182 - val_accuracy: 0.4968\n",
      "Epoch 120/200\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 1.9267e-05 - accuracy: 1.0000 - val_loss: 5.2380 - val_accuracy: 0.5065\n",
      "Epoch 121/200\n",
      "39/39 [==============================] - 1s 13ms/step - loss: 1.8672e-05 - accuracy: 1.0000 - val_loss: 5.2452 - val_accuracy: 0.5032\n",
      "Epoch 122/200\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 1.8106e-05 - accuracy: 1.0000 - val_loss: 5.2625 - val_accuracy: 0.4968\n",
      "Epoch 123/200\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 1.7438e-05 - accuracy: 1.0000 - val_loss: 5.2749 - val_accuracy: 0.5000\n",
      "Epoch 124/200\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 1.6939e-05 - accuracy: 1.0000 - val_loss: 5.2856 - val_accuracy: 0.4935\n",
      "Epoch 125/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 1.6303e-05 - accuracy: 1.0000 - val_loss: 5.2961 - val_accuracy: 0.5000\n",
      "Epoch 126/200\n",
      "39/39 [==============================] - 1s 15ms/step - loss: 1.5988e-05 - accuracy: 1.0000 - val_loss: 5.3108 - val_accuracy: 0.4935\n",
      "Epoch 127/200\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 1.5365e-05 - accuracy: 1.0000 - val_loss: 5.3217 - val_accuracy: 0.4903\n",
      "Epoch 128/200\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 1.4997e-05 - accuracy: 1.0000 - val_loss: 5.3297 - val_accuracy: 0.5000\n",
      "Epoch 129/200\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 1.4432e-05 - accuracy: 1.0000 - val_loss: 5.3428 - val_accuracy: 0.4935\n",
      "Epoch 130/200\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 1.3975e-05 - accuracy: 1.0000 - val_loss: 5.3599 - val_accuracy: 0.4903\n",
      "Epoch 131/200\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 1.3673e-05 - accuracy: 1.0000 - val_loss: 5.3617 - val_accuracy: 0.5000\n",
      "Epoch 132/200\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 1.3305e-05 - accuracy: 1.0000 - val_loss: 5.3726 - val_accuracy: 0.5000\n",
      "Epoch 133/200\n",
      "39/39 [==============================] - 0s 13ms/step - loss: 1.2811e-05 - accuracy: 1.0000 - val_loss: 5.3962 - val_accuracy: 0.4903\n",
      "Epoch 134/200\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 1.2534e-05 - accuracy: 1.0000 - val_loss: 5.4119 - val_accuracy: 0.4871\n",
      "Epoch 135/200\n",
      "39/39 [==============================] - 0s 9ms/step - loss: 1.2081e-05 - accuracy: 1.0000 - val_loss: 5.4214 - val_accuracy: 0.4903\n",
      "Epoch 136/200\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 1.1718e-05 - accuracy: 1.0000 - val_loss: 5.4308 - val_accuracy: 0.4871\n",
      "Epoch 137/200\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 1.1287e-05 - accuracy: 1.0000 - val_loss: 5.4371 - val_accuracy: 0.4935\n",
      "Epoch 138/200\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 1.0950e-05 - accuracy: 1.0000 - val_loss: 5.4591 - val_accuracy: 0.4871\n",
      "Epoch 139/200\n",
      "39/39 [==============================] - 0s 10ms/step - loss: 1.0660e-05 - accuracy: 1.0000 - val_loss: 5.4679 - val_accuracy: 0.4871\n",
      "Epoch 140/200\n",
      "39/39 [==============================] - 0s 9ms/step - loss: 1.0418e-05 - accuracy: 1.0000 - val_loss: 5.4869 - val_accuracy: 0.4903\n",
      "Epoch 141/200\n",
      "39/39 [==============================] - 0s 9ms/step - loss: 1.0071e-05 - accuracy: 1.0000 - val_loss: 5.4857 - val_accuracy: 0.4871\n",
      "Epoch 142/200\n",
      "39/39 [==============================] - 0s 9ms/step - loss: 9.7451e-06 - accuracy: 1.0000 - val_loss: 5.5089 - val_accuracy: 0.4903\n",
      "Epoch 143/200\n",
      "39/39 [==============================] - 0s 9ms/step - loss: 9.5503e-06 - accuracy: 1.0000 - val_loss: 5.5229 - val_accuracy: 0.4871\n",
      "Epoch 144/200\n",
      "39/39 [==============================] - 0s 9ms/step - loss: 9.2196e-06 - accuracy: 1.0000 - val_loss: 5.5278 - val_accuracy: 0.4871\n",
      "Epoch 145/200\n",
      "39/39 [==============================] - 0s 9ms/step - loss: 8.9538e-06 - accuracy: 1.0000 - val_loss: 5.5389 - val_accuracy: 0.4903\n",
      "Epoch 146/200\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 8.7361e-06 - accuracy: 1.0000 - val_loss: 5.5514 - val_accuracy: 0.4968\n",
      "Epoch 147/200\n",
      "39/39 [==============================] - 0s 11ms/step - loss: 8.5106e-06 - accuracy: 1.0000 - val_loss: 5.5639 - val_accuracy: 0.4839\n",
      "Epoch 148/200\n",
      "39/39 [==============================] - 0s 11ms/step - loss: 8.1412e-06 - accuracy: 1.0000 - val_loss: 5.5897 - val_accuracy: 0.4871\n",
      "Epoch 149/200\n",
      "39/39 [==============================] - 0s 11ms/step - loss: 7.9854e-06 - accuracy: 1.0000 - val_loss: 5.5868 - val_accuracy: 0.4903\n",
      "Epoch 150/200\n",
      "39/39 [==============================] - 0s 11ms/step - loss: 7.7316e-06 - accuracy: 1.0000 - val_loss: 5.6075 - val_accuracy: 0.4871\n",
      "Epoch 151/200\n",
      "39/39 [==============================] - 0s 11ms/step - loss: 7.4951e-06 - accuracy: 1.0000 - val_loss: 5.6177 - val_accuracy: 0.4839\n",
      "Epoch 152/200\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 7.3169e-06 - accuracy: 1.0000 - val_loss: 5.6276 - val_accuracy: 0.4839\n",
      "Epoch 153/200\n",
      "39/39 [==============================] - 0s 11ms/step - loss: 7.0394e-06 - accuracy: 1.0000 - val_loss: 5.6407 - val_accuracy: 0.4871\n",
      "Epoch 154/200\n",
      "39/39 [==============================] - 0s 11ms/step - loss: 6.8758e-06 - accuracy: 1.0000 - val_loss: 5.6522 - val_accuracy: 0.4871\n",
      "Epoch 155/200\n",
      "39/39 [==============================] - 0s 11ms/step - loss: 6.7370e-06 - accuracy: 1.0000 - val_loss: 5.6641 - val_accuracy: 0.4871\n",
      "Epoch 156/200\n",
      "39/39 [==============================] - 0s 11ms/step - loss: 6.5415e-06 - accuracy: 1.0000 - val_loss: 5.6729 - val_accuracy: 0.4871\n",
      "Epoch 157/200\n",
      "39/39 [==============================] - 0s 11ms/step - loss: 6.3060e-06 - accuracy: 1.0000 - val_loss: 5.6817 - val_accuracy: 0.4871\n",
      "Epoch 158/200\n",
      "39/39 [==============================] - 0s 11ms/step - loss: 6.1448e-06 - accuracy: 1.0000 - val_loss: 5.6974 - val_accuracy: 0.4903\n",
      "Epoch 159/200\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 5.9459e-06 - accuracy: 1.0000 - val_loss: 5.7070 - val_accuracy: 0.4871\n",
      "Epoch 160/200\n",
      "39/39 [==============================] - 1s 13ms/step - loss: 5.7888e-06 - accuracy: 1.0000 - val_loss: 5.7209 - val_accuracy: 0.4903\n",
      "Epoch 161/200\n",
      "39/39 [==============================] - 0s 13ms/step - loss: 5.6363e-06 - accuracy: 1.0000 - val_loss: 5.7323 - val_accuracy: 0.4871\n",
      "Epoch 162/200\n",
      "39/39 [==============================] - 0s 11ms/step - loss: 5.5191e-06 - accuracy: 1.0000 - val_loss: 5.7505 - val_accuracy: 0.4903\n",
      "Epoch 163/200\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 5.3231e-06 - accuracy: 1.0000 - val_loss: 5.7552 - val_accuracy: 0.4871\n",
      "Epoch 164/200\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 5.2126e-06 - accuracy: 1.0000 - val_loss: 5.7644 - val_accuracy: 0.4839\n",
      "Epoch 165/200\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 5.0532e-06 - accuracy: 1.0000 - val_loss: 5.7856 - val_accuracy: 0.4871\n",
      "Epoch 166/200\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 4.9360e-06 - accuracy: 1.0000 - val_loss: 5.7850 - val_accuracy: 0.4871\n",
      "Epoch 167/200\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 4.7653e-06 - accuracy: 1.0000 - val_loss: 5.8029 - val_accuracy: 0.4839\n",
      "Epoch 168/200\n",
      "39/39 [==============================] - 0s 13ms/step - loss: 4.6473e-06 - accuracy: 1.0000 - val_loss: 5.8055 - val_accuracy: 0.4871\n",
      "Epoch 169/200\n",
      "39/39 [==============================] - 1s 13ms/step - loss: 4.5103e-06 - accuracy: 1.0000 - val_loss: 5.8371 - val_accuracy: 0.4871\n",
      "Epoch 170/200\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 4.4115e-06 - accuracy: 1.0000 - val_loss: 5.8392 - val_accuracy: 0.4871\n",
      "Epoch 171/200\n",
      "39/39 [==============================] - 1s 13ms/step - loss: 4.3120e-06 - accuracy: 1.0000 - val_loss: 5.8438 - val_accuracy: 0.4839\n",
      "Epoch 172/200\n",
      "39/39 [==============================] - 1s 13ms/step - loss: 4.1684e-06 - accuracy: 1.0000 - val_loss: 5.8558 - val_accuracy: 0.4871\n",
      "Epoch 173/200\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 4.0711e-06 - accuracy: 1.0000 - val_loss: 5.8749 - val_accuracy: 0.4871\n",
      "Epoch 174/200\n",
      "39/39 [==============================] - 1s 13ms/step - loss: 3.9906e-06 - accuracy: 1.0000 - val_loss: 5.8691 - val_accuracy: 0.4903\n",
      "Epoch 175/200\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 3.8656e-06 - accuracy: 1.0000 - val_loss: 5.8899 - val_accuracy: 0.4871\n",
      "Epoch 176/200\n",
      "39/39 [==============================] - 1s 13ms/step - loss: 3.7611e-06 - accuracy: 1.0000 - val_loss: 5.9077 - val_accuracy: 0.4903\n",
      "Epoch 177/200\n",
      "39/39 [==============================] - 1s 13ms/step - loss: 3.6511e-06 - accuracy: 1.0000 - val_loss: 5.9147 - val_accuracy: 0.4871\n",
      "Epoch 178/200\n",
      "39/39 [==============================] - 0s 13ms/step - loss: 3.5569e-06 - accuracy: 1.0000 - val_loss: 5.9269 - val_accuracy: 0.4871\n",
      "Epoch 179/200\n",
      "39/39 [==============================] - 1s 14ms/step - loss: 3.4402e-06 - accuracy: 1.0000 - val_loss: 5.9371 - val_accuracy: 0.4839\n",
      "Epoch 180/200\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 3.3611e-06 - accuracy: 1.0000 - val_loss: 5.9561 - val_accuracy: 0.4839\n",
      "Epoch 181/200\n",
      "39/39 [==============================] - 1s 13ms/step - loss: 3.2822e-06 - accuracy: 1.0000 - val_loss: 5.9540 - val_accuracy: 0.4903\n",
      "Epoch 182/200\n",
      "39/39 [==============================] - 0s 11ms/step - loss: 3.2039e-06 - accuracy: 1.0000 - val_loss: 5.9793 - val_accuracy: 0.4839\n",
      "Epoch 183/200\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 3.1233e-06 - accuracy: 1.0000 - val_loss: 5.9789 - val_accuracy: 0.4839\n",
      "Epoch 184/200\n",
      "39/39 [==============================] - 1s 13ms/step - loss: 3.0232e-06 - accuracy: 1.0000 - val_loss: 6.0039 - val_accuracy: 0.4806\n",
      "Epoch 185/200\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 2.9547e-06 - accuracy: 1.0000 - val_loss: 6.0011 - val_accuracy: 0.4871\n",
      "Epoch 186/200\n",
      "39/39 [==============================] - 1s 13ms/step - loss: 2.8554e-06 - accuracy: 1.0000 - val_loss: 6.0135 - val_accuracy: 0.4839\n",
      "Epoch 187/200\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 2.7965e-06 - accuracy: 1.0000 - val_loss: 6.0248 - val_accuracy: 0.4871\n",
      "Epoch 188/200\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 2.7182e-06 - accuracy: 1.0000 - val_loss: 6.0454 - val_accuracy: 0.4839\n",
      "Epoch 189/200\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 2.6376e-06 - accuracy: 1.0000 - val_loss: 6.0428 - val_accuracy: 0.4871\n",
      "Epoch 190/200\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 2.5822e-06 - accuracy: 1.0000 - val_loss: 6.0591 - val_accuracy: 0.4839\n",
      "Epoch 191/200\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 2.5152e-06 - accuracy: 1.0000 - val_loss: 6.0714 - val_accuracy: 0.4839\n",
      "Epoch 192/200\n",
      "39/39 [==============================] - 0s 11ms/step - loss: 2.4419e-06 - accuracy: 1.0000 - val_loss: 6.0779 - val_accuracy: 0.4871\n",
      "Epoch 193/200\n",
      "39/39 [==============================] - 0s 13ms/step - loss: 2.3879e-06 - accuracy: 1.0000 - val_loss: 6.0935 - val_accuracy: 0.4839\n",
      "Epoch 194/200\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 2.3302e-06 - accuracy: 1.0000 - val_loss: 6.0945 - val_accuracy: 0.4871\n",
      "Epoch 195/200\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 2.2589e-06 - accuracy: 1.0000 - val_loss: 6.1245 - val_accuracy: 0.4871\n",
      "Epoch 196/200\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 2.2404e-06 - accuracy: 1.0000 - val_loss: 6.1248 - val_accuracy: 0.4839\n",
      "Epoch 197/200\n",
      "39/39 [==============================] - 1s 13ms/step - loss: 2.1583e-06 - accuracy: 1.0000 - val_loss: 6.1516 - val_accuracy: 0.4871\n",
      "Epoch 198/200\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 2.1166e-06 - accuracy: 1.0000 - val_loss: 6.1470 - val_accuracy: 0.4839\n",
      "Epoch 199/200\n",
      "39/39 [==============================] - 0s 12ms/step - loss: 2.0511e-06 - accuracy: 1.0000 - val_loss: 6.1585 - val_accuracy: 0.4871\n",
      "Epoch 200/200\n",
      "39/39 [==============================] - 0s 13ms/step - loss: 2.0043e-06 - accuracy: 1.0000 - val_loss: 6.1683 - val_accuracy: 0.4839\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Input(shape=(img_list[0].shape[0], img_list[0].shape[1], img_list[0].shape[2])),\n",
    "  # tf.keras.layers.BatchNormalization(),\n",
    "  tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  # tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  # tf.keras.layers.Dropout(0.3),\n",
    "  tf.keras.layers.Dense(2, activation=\"softmax\")\n",
    "])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "epochs=200\n",
    "history = model.fit(\n",
    "  x_train,\n",
    "  y_train,\n",
    "  epochs=epochs,\n",
    "  batch_size = 32,\n",
    "  validation_split = 0.2,\n",
    "  verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_history = pd.DataFrame(history.history)\n",
    "df_history.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>predict</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>388 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  predict  acc\n",
       "0        1        0    0\n",
       "1        1        0    0\n",
       "2        0        0    1\n",
       "3        1        1    1\n",
       "4        0        0    1\n",
       "..     ...      ...  ...\n",
       "383      1        1    1\n",
       "384      1        1    1\n",
       "385      0        1    0\n",
       "386      1        1    1\n",
       "387      1        1    1\n",
       "\n",
       "[388 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(x_test)\n",
    "test_results = []\n",
    "for idx, pred in enumerate(preds):\n",
    "    this_label = y_test[idx]\n",
    "    this_predict = np.argmax(pred)\n",
    "    acc = 0\n",
    "    if this_label == this_predict:\n",
    "        acc = 1\n",
    "    test_results.append({\"label\" : this_label, \"predict\" : this_predict, \"acc\" : acc})\n",
    "\n",
    "test_results = pd.DataFrame(test_results)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 정확도 : 201/388 (0.52)\n"
     ]
    }
   ],
   "source": [
    "acc_ratio = test_results[\"acc\"].sum() / len(test_results)\n",
    "print(\"테스트 정확도 : {}/{} ({:.2f})\".format(test_results[\"acc\"].sum(), len(test_results), acc_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최근 날짜로 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler_pred = Crawler(crawl_page_max=1, perPage=100) #이전 추세도 볼겸 넉넉히 수집\n",
    "crawler_pred.crawlData(cols)\n",
    "df_crawled_pred = crawler_pred.removeNan()\n",
    "df_crawled_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#아래는 예측을 위한 전처리\n",
    "dpp_pred = DataPreprocessor(df_crawled_pred.loc[:100], cols, scale_method, model_save_path)\n",
    "dpp_pred.sortByDate()\n",
    "dpp_pred.makeDiffRatio()\n",
    "dpp_pred.scalingForPredict()\n",
    "dpp_pred.makeAR(0, len_x_ARMA)\n",
    "dpp_pred.makeMA(2, len_x_ARMA)\n",
    "dpp_pred.cutoffData(len_x_ARMA, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpp_pred.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pred_list, label_pred_list, date_list, _ = makeImage2D(dpp_pred.df, cut_latest_data=False)\n",
    "len(img_pred_list), len(label_pred_list), len(date_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_day = 1\n",
    "img_pred = np.array([img_pred_list[-before_day]])\n",
    "pred = model.predict(img_pred)\n",
    "test_results = []\n",
    "predict = np.argmax(pred)\n",
    "print(\"{} predict : {}, real : {}\".format(date_list[-before_day], predict, label_pred_list[-before_day]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN + 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(img_list, y_list, test_size=0.2, shuffle=False, random_state=8699)\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Input(shape=(24, 20, 1)),\n",
    "  tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dense(20, activation=\"relu\")\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['mae', 'mse'])\n",
    "\n",
    "model.summary()\n",
    "epochs=5000\n",
    "history = model.fit(\n",
    "  x_train,\n",
    "  y_train,\n",
    "  epochs=epochs,\n",
    "  validation_split = 0.1,\n",
    "  verbose = 1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aaf533d5ae3a3d7a7fcdd7d995dbe2f3fcb3d854cc4805079aca601e58923c31"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('venv_tf_3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
